{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f844541d-674d-47e6-a940-2c4ff8692be0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,) doesn't match the broadcast shape (1,100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 204\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision Tree Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_tree, y_pred_tree))\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Run the demonstration\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m ml_algorithm_demo()\n",
      "Cell \u001b[0;32mIn[1], line 181\u001b[0m, in \u001b[0;36mml_algorithm_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m y_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m X_reg \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    180\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m LinearRegressionFromScratch()\n\u001b[0;32m--> 181\u001b[0m lr_model\u001b[38;5;241m.\u001b[39mfit(X_reg, y_reg)\n\u001b[1;32m    182\u001b[0m y_pred_reg \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mpredict(X_reg)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear Regression MSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_squared_error(y_reg, y_pred_reg))\n",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m, in \u001b[0;36mLinearRegressionFromScratch.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m     db \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mn_samples) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_predicted \u001b[38;5;241m-\u001b[39m y)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m dw\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m db\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,) doesn't match the broadcast shape (1,100)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Linear Regression\n",
    "class LinearRegressionFromScratch:\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.iterations):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1/n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "# Logistic Regression\n",
    "class LogisticRegressionFromScratch:\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1/n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "\n",
    "# Decision Tree\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "    \n",
    "    def _information_gain(self, parent, left_child, right_child):\n",
    "        parent_entropy = self._entropy(parent)\n",
    "        left_entropy = self._entropy(left_child)\n",
    "        right_entropy = self._entropy(right_child)\n",
    "        \n",
    "        n = len(parent)\n",
    "        n_l, n_r = len(left_child), len(right_child)\n",
    "        \n",
    "        # Weighted entropy\n",
    "        child_entropy = (n_l/n) * left_entropy + (n_r/n) * right_entropy\n",
    "        return parent_entropy - child_entropy\n",
    "    \n",
    "    def _split(self, X, feature_idx, threshold):\n",
    "        left_mask = X[:, feature_idx] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        return left_mask, right_mask\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_classes == 1 or \n",
    "            n_samples < 2):\n",
    "            leaf_value = np.bincount(y).argmax()\n",
    "            return {'value': leaf_value}\n",
    "        \n",
    "        # Find best split\n",
    "        best_gain = -1\n",
    "        best_feature, best_threshold = None, None\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_mask, right_mask = self._split(X, feature_idx, threshold)\n",
    "                \n",
    "                if len(left_mask) == 0 or len(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = self._information_gain(\n",
    "                    y, \n",
    "                    y[left_mask], \n",
    "                    y[right_mask]\n",
    "                )\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        # Recursively build tree\n",
    "        left_mask, right_mask = self._split(X, best_feature, best_threshold)\n",
    "        left_subtree = self._build_tree(\n",
    "            X[left_mask], \n",
    "            y[left_mask], \n",
    "            depth + 1\n",
    "        )\n",
    "        right_subtree = self._build_tree(\n",
    "            X[right_mask], \n",
    "            y[right_mask], \n",
    "            depth + 1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _predict_single(self, x, tree):\n",
    "        if 'value' in tree:\n",
    "            return tree['value']\n",
    "        \n",
    "        feature_val = x[tree['feature']]\n",
    "        if feature_val <= tree['threshold']:\n",
    "            return self._predict_single(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_single(x, tree['right'])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self._predict_single(x, self.tree) for x in X]\n",
    "\n",
    "# Demonstration of usage\n",
    "def ml_algorithm_demo():\n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Linear Regression Example\n",
    "    X_reg = np.random.rand(100, 1)\n",
    "    y_reg = 2 * X_reg + 1 + np.random.normal(0, 0.1, (100, 1))\n",
    "    y_reg = y_reg.reshape(-1)\n",
    "    lr_model = LinearRegressionFromScratch()\n",
    "    lr_model.fit(X_reg, y_reg)\n",
    "    y_pred_reg = lr_model.predict(X_reg)\n",
    "    print(\"Linear Regression MSE:\", mean_squared_error(y_reg, y_pred_reg))\n",
    "    \n",
    "    # Logistic Regression Example\n",
    "    X_class = np.random.randn(100, 2)\n",
    "    y_class = (X_class[:, 0] + X_class[:, 1] > 0).astype(int)\n",
    "    \n",
    "    log_reg = LogisticRegressionFromScratch()\n",
    "    log_reg.fit(X_class, y_class)\n",
    "    y_pred_class = log_reg.predict(X_class)\n",
    "    print(\"Logistic Regression Accuracy:\", accuracy_score(y_class, y_pred_class))\n",
    "    \n",
    "    # Decision Tree Example\n",
    "    X_tree = np.random.randn(100, 2)\n",
    "    y_tree = (X_tree[:, 0] > 0).astype(int)\n",
    "    \n",
    "    dt = DecisionTreeClassifier(max_depth=5)\n",
    "    dt.fit(X_tree, y_tree)\n",
    "    y_pred_tree = dt.predict(X_tree)\n",
    "    print(\"Decision Tree Accuracy:\", accuracy_score(y_tree, y_pred_tree))\n",
    "\n",
    "# Run the demonstration\n",
    "ml_algorithm_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
